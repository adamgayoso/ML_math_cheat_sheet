\documentclass[11pt]{article}

\usepackage[textwidth=6.5in, textheight=9.5in]{geometry}

\usepackage{amsfonts}
\usepackage{amssymb,amsmath, amsthm}
\usepackage{siunitx}
\usepackage{url, graphicx}
\usepackage{xspace}

\usepackage{bm} % Define common bold symbols with \bmdefine{cmd}{def}. Use \bm{var} ad hoc in text
\bmdefine{\bmx}{x}
\bmdefine{\bmA}{A}
\bmdefine{\bma}{a}
\bmdefine{\bmy}{y}

%%% Destinguished symbols
\def\R{{\mathbb{R}}}
\def\Given{{\,|\,}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\def\iid{{\stackrel{\text{iid}}{\sim}}}
\def\Ind{{\stackrel{\text{ind}}{\sim}}}
\DeclareMathOperator{\Trace}{tr}

%%% Math operator shortcuts
\newcommand*{\Inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand*{\Prox}[2]{\operatorname{prox}_{#1}(#2)}
\newcommand*{\fracbig}[2]{\left( \frac{#1}{#2} \right)}

%%% Other special abreviation defs

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\roman{subsubsection}}

\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{cleveref} % Needs to be last

%opening
\title{ML \& LinAlg Math Cheat Sheet}

\begin{document}

\maketitle
\tableofcontents

\section{Notation}
Vectors are column vectors denoted by lower-case bolded variables, such that 
\[\bmx = \left[ \begin{split}
x_1 \\ \vdots \\ x_N 
\end{split} \right]. \]
A row vector is denoted $\bmx^\top = [x_1 \ldots x_N]$.
A matrix is indicated by a bolded upper-case variable, such that an $N \times M$ matrix is
\begin{align*}
\bmA = \{a_{ij}\} = [\bma_1 \cdots \bma_M ] =
\left[\begin{array}{c}
\bma_1^\top \\
\vdots \\
\bma_N^\top
\end{array}\right]
=
\left[\begin{array}{ccc}
a_{1,1}^\top & \cdots & a_{1, M} \\
\vdots & \ddots & \vdots \\
a_N^\top & \cdots & a_{N, M}
\end{array}\right].
\end{align*}

\section{Derivative}
\subsection{Vector Gradient}
\begin{equation}
\nabla_{\bmx} \bmy = [\frac{\partial \bmy}{\partial x_1}, \ldots, \frac{\partial \bmy}{\partial x_N}]
\label{eq:Derivative-random_vector}
\end{equation}

\section{Determinant Operator}
\subsection{Random Properties}
For scalar $c$ and $N\times N$ identity matrix $I$,
\[ \det(cI) = c^N. \]

\section{Trace Operator}
\subsection{Derivatives}
\subsubsection{$\nabla_{\bmx}\Trace(\bmx \bmx^\top \bmA) = 2\bmx^\top \bmA$}
For square $N\times N$ matrix $\bmA$,
\begin{align*}
\nabla_{\bmx}\Trace(\bmx \bmx^\top \bmA) =
\frac{d}{d \bmx}\Trace(\bmx \bmx^\top \bmA) &=
\frac{d}{d \bmx} \sum_{i}^{N} \sum_{k}^{N} x_i x_k a_{ik}.
\end{align*}
Recall \cref{eq:Derivative-random_vector}, and consider for any $j \in \{1,\ldots, N\}$:
\begin{align*}
\frac{\partial}{\partial x_j} \sum_{i}^{N} \sum_{k}^{N} x_i x_k a_{ik} &=
x_1a_{j,1} + x_2a_{j,2} + \cdots \frac{\partial}{\partial x_j} \sum_{k}^{N} x_j x_k a_{jk} + \cdots x_N a_{j, N} \\
&= \sum_i^N x_ia_{ji} - x_ja_{jj} + \sum_{k}^{N} x_k a_{jk} - x_ja_{jj} + \frac{\partial}{\partial x_j} x_j x_j a_{jj} \\
&= 2\sum_i^N x_ia_{ji} - 2 x_ja_{jj} + 2 x_ja_{jj} \\
&= 2\bmx^\top \bma_j.
\end{align*}

This equally applies for any $j$ in $1\ldots N$, and so for the full gradient:
\begin{align}
\nabla_{\bmx}\Trace(\bmx \bmx^\top \bmA) =
\frac{d}{d \bmx} \sum_{i}^{N} \sum_{k}^{N} x_i x_k a_{ik} &=
[2 \bmx^\top \bma_1 \cdots 2\bmx^\top \bma_N] = 2\bmx^\top \bmA.
\label{eq:Trace-Derivative-tr(xx'A)}
\end{align}

\subsection{Relation to Determinant}



\end{document}
